{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "#from tensorflow.keras import layers, models\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = np.load(\"outcome.npy\")\n",
    "specs = np.load(\"specs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_specs =  librosa.util.normalize(specs, norm=2, axis=1, threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalised_specs\n",
    "y = outcome\n",
    "\n",
    "#X = np.vstack(X)\n",
    "X = np.expand_dims(X, -1)\n",
    "y = np.expand_dims(y, -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 128, 79, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_reshape(input):\n",
    "    out = []\n",
    "    for i in range(len(input)):\n",
    "        if input[i][0] == True: #normal is true\n",
    "            out.append(0) #0\n",
    "        else:\n",
    "            out.append(1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, X_train, X_test, y_train, y_test, threshold = 0.5):\n",
    "    score = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print(\"Train set:\")\n",
    "    print('Train loss:', score[0])\n",
    "    print('Train accuracy:', score[1])\n",
    "    \n",
    "    pred_proba_train = model.predict(X_train)\n",
    "    pred_train = tf.greater(pred_proba_train, threshold)\n",
    "    pred_train_reshape = outcome_reshape(pred_train)\n",
    "    y_train_reshape = outcome_reshape(y_train)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train_reshape, pred_train_reshape, labels = [0,1]).ravel()\n",
    "    print(\"tn:\", tn, \"fp:\", fp, \"fn:\", fn, \"tp:\", tp )\n",
    "    print(\"Sensitivity:\", tp/(tp+fn), \"| Specificity:\", tn/(tn+fp))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Test set:\")\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    pred_proba_test = model.predict(X_test)\n",
    "    pred_test = tf.greater(pred_proba_test, threshold)\n",
    "    pred_test_reshape = outcome_reshape(pred_test)\n",
    "    y_test_reshape = outcome_reshape(y_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_reshape, pred_test_reshape, labels = [0,1]).ravel()\n",
    "    print(\"tn:\", tn, \"fp:\", fp, \"fn:\", fn, \"tp:\", tp )\n",
    "    print(\"Sensitivity:\", tp/(tp+fn), \"| Specificity:\", tn/(tn+fp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "##model building\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "num_category = 2\n",
    "model = Sequential()\n",
    "#convolutional layer with rectified linear unit activation\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#64 convolution filters used each of size 3x3\n",
    "#choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#randomly turn neurons on and off to improve convergence\n",
    "model.add(Dropout(0.25))\n",
    "#flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "#fully connected to get all relevant data\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#one more dropout for convergence' sake :) \n",
    "model.add(Dropout(0.5))\n",
    "#output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(num_category, activation='sigmoid'))\n",
    "\n",
    "#Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "#categorical ce since we have multiple classes (10) \n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 966ms/step - accuracy: 0.5000 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
      "Epoch 2/4\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 997ms/step - accuracy: 0.5000 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/4\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 974ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 4/4\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 925ms/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6928\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epoch = 4\n",
    "#model training\n",
    "model_log = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "Train loss: 0.6928572654724121\n",
      "Train accuracy: 0.5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
      "tn: 471 fp: 106 fn: 94 tp: 17\n",
      "Sensitivity: 0.15315315315315314 | Specificity: 0.8162911611785095\n",
      "\n",
      "\n",
      "Test set:\n",
      "Test loss: 0.6927960515022278\n",
      "Test accuracy: 0.5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "tn: 122 fp: 27 fn: 19 tp: 4\n",
      "Sensitivity: 0.17391304347826086 | Specificity: 0.8187919463087249\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(model, X_train, X_test, y_train, y_test, threshold = 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal:  111 normal:  577\n"
     ]
    }
   ],
   "source": [
    "print(\"abnormal: \", sum(y_train_reshape), \"normal: \", len(y_train_reshape) - sum(y_train_reshape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "##model building\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "num_category = 2\n",
    "model = Sequential()\n",
    "#convolutional layer with rectified linear unit activation\n",
    "model.add(Conv2D(20, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 strides= (1, 1),\n",
    "                 input_shape=input_shape))\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "\n",
    "#64 convolution filters used each of size 3x3\n",
    "#choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#randomly turn neurons on and off to improve convergence\n",
    "model.add(Conv2D(50, (5, 5), strides= (1, 1), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "#fully connected to get all relevant data\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "#output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(num_category, activation='sigmoid'))\n",
    "\n",
    "#Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "#categorical ce since we have multiple classes (10) \n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', \n",
    "            #   tf.keras.metrics.FalsePositives(),\n",
    "            #   tf.keras.metrics.FalseNegatives(),\n",
    "            #   tf.keras.metrics.TruePositives(),\n",
    "            #   tf.keras.metrics.TrueNegatives(),\n",
    "              'BinaryAccuracy',\n",
    "              'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - BinaryAccuracy: 0.5103 - accuracy: 0.5000 - loss: 0.6932 - recall: 0.0880 - val_BinaryAccuracy: 0.6512 - val_accuracy: 0.5000 - val_loss: 0.6931 - val_recall: 0.3547\n",
      "Epoch 2/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 222ms/step - BinaryAccuracy: 0.6361 - accuracy: 0.5000 - loss: 0.6930 - recall: 0.3727 - val_BinaryAccuracy: 0.7093 - val_accuracy: 0.5000 - val_loss: 0.6929 - val_recall: 0.5174\n",
      "Epoch 3/3\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - BinaryAccuracy: 0.7208 - accuracy: 0.5000 - loss: 0.6929 - recall: 0.5519 - val_BinaryAccuracy: 0.7529 - val_accuracy: 0.5000 - val_loss: 0.6927 - val_recall: 0.6163\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epoch = 3\n",
    "#model training\n",
    "model_log = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "Train loss: 0.692749559879303\n",
      "Train accuracy: 0.7434592843055725\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "tn: 423 fp: 154 fn: 85 tp: 26\n",
      "Sensitivity: 0.23423423423423423 | Specificity: 0.733102253032929\n",
      "\n",
      "\n",
      "Test set:\n",
      "Test loss: 0.6927259564399719\n",
      "Test accuracy: 0.7529069781303406\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "tn: 106 fp: 43 fn: 17 tp: 6\n",
      "Sensitivity: 0.2608695652173913 | Specificity: 0.7114093959731543\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(model, X_train, X_test, y_train, y_test, threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abnormal:  111 normal:  577\n"
     ]
    }
   ],
   "source": [
    "print(\"abnormal: \", sum(y_train_reshape), \"normal: \", len(y_train_reshape) - sum(y_train_reshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
